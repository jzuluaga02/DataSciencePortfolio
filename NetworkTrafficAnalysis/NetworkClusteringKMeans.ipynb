{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python pyspark==3.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypFJhFq4Vemz",
        "outputId": "ab0a483e-2b7d-48f9-b73f-1a9889bec826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyspark==3.2.0 in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.2.0) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install confluent-kafka\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUWNs0jJ0ayw",
        "outputId": "11d23d15-eba9-4b3a-836f-216f3eef1260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: confluent-kafka in /usr/local/lib/python3.10/dist-packages (2.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n"
      ],
      "metadata": {
        "id": "eib7dMKx0iHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNvcGYx1VkwF",
        "outputId": "59df6e43-3798-44d4-d845-84f11781b182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Credential to confluent-cloud\n",
        "bootstrap_servers = 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092'\n",
        "security_protocol = 'SASL_SSL'\n",
        "sasl_mechanism = 'PLAIN'\n",
        "sasl_plain_username = '7LSJTV6ECYBKIDMH'\n",
        "sasl_plain_password = '6jLYdOnP+vH8msoYrWfsZfbxdrtWgsrod1nLVZGYJqHkMxQO49r4+ifTVnOW4A5M'"
      ],
      "metadata": {
        "id": "CCZPqv-P0k64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "scala_version = '2.12'  # TODO: Ensure this is correct\n",
        "spark_version = '3.2.1'\n",
        "packages = [\n",
        "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
        "    'org.apache.kafka:kafka-clients:3.2.0'\n",
        "]\n",
        "spark = SparkSession.builder\\\n",
        "   .master(\"local\")\\\n",
        "   .appName(\"kafka-example\")\\\n",
        "   .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
        "   .getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Cn6Cc8QYyvva",
        "outputId": "910d5b32-c10a-4171-d523-df4c3995f869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9184b318d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://11fb53720739:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>kafka-example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxQkYobDU3E1"
      },
      "outputs": [],
      "source": [
        "from kafka import KafkaProducer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import re\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark = SparkSession.builder.appName('RequestClustering').getOrCreate()\n",
        "\n",
        "\n",
        "# most optimal feature for clustering\n",
        "feature_columns = ['host', 'time', 'status_code', 'data_size']\n",
        "\n",
        "# Assemble the features into a feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n"
      ],
      "metadata": {
        "id": "mi9VWPfmV5m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Define the clustering algorithm\n",
        "k = 5 # most optiomal number of clusters\n",
        "kmeans = KMeans(featuresCol='features', k=k)\n",
        "\n",
        "# Function to calculate similarity between features and cluster centers\n",
        "def calculate_similarity_func(features, cluster_centers):\n",
        "    distances = [np.linalg.norm(features - center) for center in cluster_centers]\n",
        "    return min(distances)\n",
        "\n",
        "# Define a UDF to calculate similarity between a request and the cluster center\n",
        "calculate_similarity = udf(lambda features: calculate_similarity_func(features, cluster_centers), DoubleType())\n",
        "\n"
      ],
      "metadata": {
        "id": "Cuo30nUVWBPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from the log file into a DataFrame, use regular expressions to clean data\n",
        "log_file = './42MBSmallServerLog.log'\n",
        "log_pattern = r'(\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\d+) (\\d+)'\n",
        "log_data = []\n",
        "\n",
        "with open(log_file, 'r') as file:\n",
        "    for line in file:\n",
        "        match = re.match(log_pattern, line)\n",
        "        if match:\n",
        "            log_data.append((match.group(1), match.group(2), match.group(4), match.group(5)))\n",
        "\n",
        "requests_df = spark.createDataFrame(log_data, schema='host STRING, time STRING, status_code STRING, data_size STRING')"
      ],
      "metadata": {
        "id": "0thT4RB6Wy4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "requests_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REOx25yawMRy",
        "outputId": "8852750d-87bf-48f4-a80e-e9e30088907c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+-----------+---------+\n",
            "|           host|                time|status_code|data_size|\n",
            "+---------------+--------------------+-----------+---------+\n",
            "|    .236.105.42|26/Dec/2118:12:00...|        500|    13359|\n",
            "| 220.69.190.164|26/Dec/2118:12:00...|        304|    34734|\n",
            "| 33.208.243.254|26/Dec/2118:12:00...|        500|    27982|\n",
            "|  177.70.88.105|26/Dec/2118:12:00...|        500|    24106|\n",
            "|  10.237.41.225|26/Dec/2118:12:00...|        200|    26324|\n",
            "| 134.56.148.101|26/Dec/2118:12:00...|        403|     1376|\n",
            "|  103.57.36.200|26/Dec/2118:12:00...|        500|     5130|\n",
            "|    15.177.4.33|26/Dec/2118:12:00...|        200|    32155|\n",
            "|188.157.130.200|26/Dec/2118:12:00...|        403|    57425|\n",
            "|    1.59.51.230|26/Dec/2118:12:00...|        303|    20113|\n",
            "| 199.192.22.229|26/Dec/2118:12:00...|        200|     5927|\n",
            "|  95.77.174.217|26/Dec/2118:12:00...|        404|     9489|\n",
            "| 213.218.154.74|26/Dec/2118:12:00...|        403|    39723|\n",
            "|  141.78.203.63|26/Dec/2118:12:00...|        200|     4000|\n",
            "|   50.53.184.42|26/Dec/2118:12:00...|        404|    28408|\n",
            "|  57.136.47.204|26/Dec/2118:12:00...|        303|    25127|\n",
            "|   39.50.253.57|26/Dec/2118:12:00...|        404|    49051|\n",
            "|  22.191.137.59|26/Dec/2118:12:00...|        500|    56314|\n",
            "| 188.215.96.180|26/Dec/2118:12:00...|        200|      296|\n",
            "| 176.94.212.169|26/Dec/2118:12:00...|        200|    13552|\n",
            "+---------------+--------------------+-----------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kafka_bootstrap_servers = bootstrap_servers\n",
        "kafka_topic = 'logs'\n",
        "producer_config = {\n",
        "    'bootstrap.servers': kafka_bootstrap_servers,\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': sasl_plain_username,\n",
        "    'sasl.password': sasl_plain_password\n",
        "}\n",
        "\n",
        "producer = Producer(producer_config)\n",
        "\n",
        "batch_size = 1000  # Number of requests to batch together\n",
        "batch = []  # List to store batched requests\n",
        "\n",
        "for row in requests_df.collect():\n",
        "    request = ','.join([str(elem) for elem in row])\n",
        "    batch.append(request.encode('utf-8'))\n",
        "    \n",
        "    if len(batch) >= batch_size:\n",
        "        # Serialize the batched requests into a single bytes-like object\n",
        "        serialized_batch = b'\\n'.join(batch)\n",
        "        \n",
        "        # Send the serialized batched requests in a single produce call\n",
        "        producer.produce(kafka_topic, value=serialized_batch)\n",
        "        producer.flush()\n",
        "        batch = []  \n",
        "\n",
        "# Send any remaining requests in the batch\n",
        "if batch:\n",
        "    serialized_batch = b'\\n'.join(batch)\n",
        "    producer.produce(kafka_topic, value=serialized_batch)\n",
        "    producer.flush()\n",
        "\n",
        "producer.close()\n"
      ],
      "metadata": {
        "id": "qh-NmZjbwXa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform clustering on the requests DataFrame\n",
        "feature_vector = assembler.transform(requests_df)\n",
        "model = kmeans.fit(feature_vector)\n",
        "cluster_centers = model.clusterCenters()"
      ],
      "metadata": {
        "id": "Ghw5DxQ-dw73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster assign\n",
        "requests_df = requests_df.withColumn('cluster', calculate_similarity('features'))\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "2ArqJt4QeIjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}